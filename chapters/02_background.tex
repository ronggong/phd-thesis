\chapter{Background}\label{chap:bkgnd}

\section{Jingju music}
Jingju (also known as Beijing or Peking opera) is the most representative form of Chinese opera which assimilates the essence of various Chinese opera forms such as 徽剧 (Anhui opera), 昆曲 (Kun opera), 秦腔 (Qin qiang) and 高腔 (Gao qiang). It arose in the late 18th century and became fully developed in the mid-19th century. Now it is regarded one of the cultural treasures in China and inscribed in the UNESCO representative list of the intangible cultural heritage of humanity. Jingju is widely practised over mainland China, Hong Kong, Taiwan, and overseas countries where there is Chinese communities presence. Major jingju performing troupes are located in big mainland China cities such as Beijing, Tianjin and Shanghai. A significant amount of jingju musicological literature can be used to formulate MIR problems. The presence of a large audience and musicological literature are an important motivation to carry a computational research for this music culture.

This section describes the focus of this dissertation -- jingju music culture. The emphasis is on singing concepts in this music culture. This section is not a comprehensive introduction to this culture but is aimed to be sufficient to support the following chapters of the dissertation.

We use simplified Chinese characters (computer encoding: GB2312) to introduce jingju terminologies for the first time in this dissertation. We also introduce the pinyin, the romanization system of Mandarin Chinese, for each terminology. Only the pinyin form of the terminology will be used throughout the dissertation. 

\subsection{A synthetic art form}

Professor Li in National Academy of Chinese Theatre Arts (NACTA) said ``The three basic elements of Chinese opera are 曲 (pinyin: qu, tune), 程式 (pinyin: chengshi, conventions -- a strict set of rules) and 虚拟表演 (pinyin: xu ni biao yan, virtual acting). These three elements are ultimately aiming to support 戏 (pinyin: xi), which can be approximately understood as `entertainment'." Of the three elements, ``tune" is the most important one, which represents all the musical dimensions of the jingju music. However, this representation is not only limited to music but constructs the whole skeleton of the jingju performing.

Jingju is a synthetic art form which includes four disciplines -- 唱 (pinyin: chang, singing)，念 (pinyin: nian, declamation)，做 (pinyin: zuo, physical acting) and 打 (pinyin: da, acrobatics). Singing is directly related to tune, and the other three disciplines are integrated together by the music and rhythm of jingju performing. 

The jingju technical training for performers consists in becoming proficient of the conventions of the four disciplines as mentioned earlier which are established by tradition. The jingju performers use these conventions to construct characters and convey stories. For example, they use singing conventions to express the character's emotional state. The jingju performance is codified through the conventions which are not aimed at hinder the creativity and artistry. The appreciation of the beauty of jingju is to see how the performers are conveying the conventions. In jingju training, a performer will have more creativity if she/he can master more conventions.

\subsection{Singing and instrumental accompaniment}

``In the aural performance of Beijing opera, two types of sounds are actually heard: song and speech vocalized by the stage performers, and instrumental music played by the musicians of the orchestra. The voice of the Beijing opera performer, is the featured component of aural performance." -- Elizabeth Wichmann.

In a jingju play, the sections where singing occurs are 唱段 (pinyin: chang duan, literally translated as singing section). The closest form to chang duan in Western opera is "aria", which signifies "any closed lyrical piece for solo voice (exceptionally for more than one voice) with or without instrumental accompaniment, either independent or forming part of an opera, oratorio, cantata or other large work." The difference between chang duan and aria is that latter is a self-sufficient piece conceptually, whereas chang duan is formulated in a dramatic continuum, although it is usually performed and recorded individually [ref Rafa thesis]. 

Jingju chang duan is started actually before the performer starts to sing. The declaration of the starting point of a chang duan is 叫板 (pinyin: jiaoban, literally translated as "calling the banshi"). Banshi is the rhythmic framework concept that we will introduce it in \secref{sec:banshi}. Jiaoban is included in every chang duan of the commercial recordings and teached in conservatory jingju performing classes. The percussion pattern 住头 (pinyin: zhu tou) is to signal the end of a chang duan.

Jingju instrumental ensemble is divided into two sections -- 文场 (pinyin: wenchang, literally translated as "civil scene") and 武场 (pinyin: wuchang, literally translated as "martial scene"). Wenchang is the orchestral accompaniment, and wuchang is formed by percussion instruments. There are five basic percussion instruments -- 单皮鼓 (pinyin: danpigu, drum), 板 (pinyin: ban, clappers), 铙钹 (pinyin: naobo, cymbals), 大锣 (pinyin: daluo, big gong), 小锣 (pinyin: xiaoluo, small gong). The first two instruments are played normal by the same person, so they have a combined term -- bangu. The musician who plays the bangu is called 司鼓 (pinyin: si gu), literally translated as "the man who is in charge of the bangu". The primary functions of wuchang are playing 锣鼓经 (pinyin: luo gu jing, rhythm patterns) and supporting the rhythmic aspect of the actor/actress’ performing.

The main instrument of wenchang is 京胡 (pinyin: jinghu). Having loud volume, and very bright and penetrating sound, jinghu is the aural representative of jingju sound. The musician who plays jinghu is called 琴师 (pinyin: qin shi, master instrumentalist). The major role played by qinshi is supporting the jingju melody. Traditionally, qinshi is the musician who has the closest collaboration with the performer. Jingju line sustains the singing line to form an uninterrupted melody stream, which impels the singing. 

The other instruments in wenchang are 月琴 (pinyin: yueqin), 三弦 (pinyin: sanxian), 京二胡 (pinyin: jingerhu), 阮 (pinyin: ruan), 中阮 (pinyin: zhongruan) and 大阮 (pinyin: daruan). They all play the same melody as the jinghu line in the same or different octave, and in a heterophonic structure. The performer, sigu and qinshi take turns in coordinating the jingju performing tempo.

\subsection{Lyrics structure}

The primary function of the lyrics in jingju is telling stories. Music structure in jingju is closely related to lyrics structure. The tune sequences in jingju are inherited from the creation principle in poetry of Tang dynasty (618 - 907 AC), that the melody and poetic structure are taken from the preexisting poems or songs, and new lyrics are arranged to fit in that schema. The new lyrics are labelled with the name of the original poem or song, so that the performer knows how to sing the tune. The label of the original poem or song is called 曲牌 (pinyin: qu pai, literally translated as tune label). Different qupai have different forms which represent not only different melodies but also a different number of melodic lines and a different number of characters in each line. This kind of lyrics structure is called 长短句 (pinyin: chang duan ju, literally translated as long and short lines). A jingju chang duan consists of a sequence of these chang duan ju.

The basic structure of lyrics stanza consists of two symmetrical lines which have the same number of characters. The most common term in jingju circles for describing this two symmetrical lines is 上下句 (pinyin: shang xia ju, literally translated as upper and lower lines). The most common English terminology for shang xia ju is couplet for the stanze, opening line for upper line and closing line for lower line. A standard line has either 7 or 10 characters, grouped in three sections -- 2 + 2 + 3 or 3 + 3 + 4. These sections, namely 逗 (pinyin: dou), are the basic semantic and rhythmic units.

The lyrics structure mentioned above can be modified in actual singing. A typical case is the variation of the number of characters in each line, for example, 衬字 (pinyin: chenzi), the characters do not have semantic meaning, but serve to help the performer prolong the singing of certain nasal or vowel sounds. Another form of increasing the number of characters is 垛字 (pinyin: duozi), which inserts semantic units containing 3 or 4 characters into the line.

\subsection{Linguistic tones and pronunciation}

It is commonly assumed that the linguistic intonation of a tonal language singing needs to agree with its melody to a certain extent to make sure the intelligibility. For jingju which is sung by using mainly Chinese Mandarin language and various dialects, its music features are related to the dialects. In other words, The Chinese dialects used in jingju singing determines its melody characteristics to a certain extent.

In jingju circles, it exists an expression to describe the relation between linguistic tones and melody -- 字正腔圆 (pinyin: zi zheng qiang yuan, literally translated as “characters should be straight, tune should be round.”). This expression can be understood as that the performer needs to attain both the intelligibility of the lyrics and the smooth sounding of the melody. The most critical problem to be avoided in jingju singing is 倒字 (pinyin: dao zi, literally translated as upside-down character), which means that the lyrics is misunderstood because the performer mispronounces certain characters.

Most of the jingju scholars agree in that jingju singing uses mainly two Chinese dialects -- 北京音 (pinyin: Beijing yin, the dialect of Beijing) and 湖广音 (pinyin: Huguang yin, the dialect of Huguang). Some scholars consider that jingju singing also uses a third Chinese dialect -- 中州韵 (pinyin: Zhongzhou yun, literally translated as rhymes from Zhongzhou). All three dialects share the same tone categories 阴平 (yin ping), 阳平 (yang ping), 上 (shang) and 去 (qu), although the pitch contours of the same characters realized in the same categories are different for the three dialects.

The three dialects result in the complexity of the pronunciation in jingju singing. Such complexity influences the linguistic tones as well as the pronunciation of the syllabic initials and finals. The standard Chinese used in Mainland China -- 普通话 (pinyin: pu tong hua), very close to Beijing yin, is taken as the reference for jingju pronunciation. All the special pronunciations different from the reference putonghua can be divided into two categories -- 尖团字 (pinyin: jian tuan zi, literally translated as pointed and rounded characters) and 上口字 (pinyin: shang kou zi, literally "up to the mouth" characters). Jian tuan zi has two sub-categories of characters -- 尖字 (pinyin: jianzi) and 团字 (pinyin: tuanzi), which are separated by the fricative and affricative consonants of a syllable. When studying a new play, jingju performer should learn which characters belonging to tuanzi in putonghua should be pronounced as jianzi. The jian tuan zi qualities are considered extremely important for both listening comprehension and aesthetic effect. Shang kou zi are generally a set of characters of which the pronunciation is different from the standard Mandarin, adopting from southern Chinese dialects -- Huguang yin and Zhongzhou yun. By shang kou zi and converting certain tuanzi to jianzi, the language of jingju is made more appealing to speakers of the diverse range of dialects throughout China than is Mandarin alone. Jian tuan zi and shang kou zi are one of the main study focuses of this dissertation. Thus a more specific extended description will be presented in section. 

\subsection{Role-types}

行当 (pinyin: hang dang), commonly translated as role-type, is a colloquial term for the “acting profiles” of a jingju performing character. There are four role-types in jingju -- 生 (sheng), 旦 (dan), 净 (jing) and 丑 (chou), which respectively have their specific styles of performing, speaking, singing, costume, and make-up. These oral and visual means of expression define the gender, approximate age, social status, profession, personality and singing style. Due to the various and complicate conventions that each role-type possesses, every performer has to specialize one role-type and practice these conventions along the performing career. 

\begin{table}[ht!]
\centering
\begin{tabular}{l|cccc}
\toprule
Main role-types & sheng               & dan                           & jing & chou \\
\midrule
\multirow{4}{*}{Sub role-types} & laosheng\textsuperscript{*} & qingyi\textsuperscript{*} & tongchui    & wenchou   \\
& xiaosheng & huadan & jiazi & wuchou \\
&&laodan&& \\
&&wudan&& \\
\bottomrule
\end{tabular}
\caption{Jingju four role-types and their sub role-types. The role-types with * superscript are the main research objects of this dissertation because singing is their major discipline.}
\label{tab:role-types}
\end{table}

Sheng role-type is specialized in the performance of male characters, whereas dan role-type is specialized in that of female characters. Jing role-type depicts the male characters with an exaggerated temperament. Chou role-type is used for male or female comic characters. The most obvious difference between the male’s voice and the female’s voice is the timbre. Male role-types sing with chest voice, while female role-types use falsetto. Regarding the singing pitch register, there is a displacement of the pitch range in the female singing to a higher region, where female sings a fourth to an octave higher than male singing. Regarding melodic contours, female singing is usually more melismatic than male singing. 

老生 (pinyin: lao sheng) role-type portrays adult or old male characters, which is also the representative of male singing. All textbooks use the examples of laosheng role-type to explain elements of jingju music system. Two representative sub-role-types in dan are 青衣 (pinyin: qing yi) and 花旦 (pinyin: hua dan). The former is most representative role-type of female singing, and generally used for building female characters from a higher social classes. The latter is used for building female characters with a playful personality.

We list the major jingju role-types in \tabref{tab:role-types}, where laosheng and qingyi are the main research objects of this dissertation since singing is their major discipline.

\subsection{Shengqiang}

There is no a coherent definition of jingju 声腔 (pinyin: shengqiang) between scholars. Some of them define shengqiang as tune families of jingju music, meaning a tune which has been evolved into different versions in the performing and transmission process throughout the history. Although these tunes share certain tonal, modal, and dramatic function, they tend to differ from each other in metrical, rhythmic, and melodic details. The shengqiang definition of Elizabeth Wichmann deviated from tune family, and characterize a group of related shengqiang as system. Each shengqiang system is identified by its unique patterns of modal rhythm, song structure, melodic contour and construction, and keys and cadences.  

Jingju contains mainly eight shengqiang -- 西皮 (xipi), 二黄 (erhuang), 反西皮 (fanxipi), 反二黄 (fanerhuang), 四平调 (sipingdiao), 南梆子 (nanbangzi), 高拨子 (gaobozi) and 吹腔 (chuiqiang). Two shengqiang with the most significant presence in jingju arias are xipi and erhuang. Fanxipi and fanerhuang are respectively first degree shifted versions of xipi and erhuang. Additionally, shengqiang is related to the emotional content of the aria. For example, Wichmann describes the emotional content of erhuang as dark, deep, profound, heavy and meticulous, and xipi as sprightly, bright, clear, energetic, forceful, purposeful. 

\subsection{Banshi}\label{sec:banshi}

Ban means the percussion instrument clappers used in jingju wuchang. Banshi can be understood as the jingju rhythmic patterns.  There are four types of banshi in jingju -- 一板一眼 (one ban and one eye), 一板三眼 (one ban and three eyes), 有板无眼 (ban but no eye) and 无板无眼 (no ban and no yan), where ban and eye indicate respectively accented and unaccented beats. The first three banshi are metred types, and usually notated in jianpu scores correspondingly with the time signatures 2/4, 4/4 and 1/4. The last is assigned to 散板 (pinyin: sanban), meaning unmetred type. Wichmann describes that each banshi has a characteristic tempo, is associated with certain characteristic melodic tendencies, and is perceived as appropriate for certain dramatic situations. 

\begin{table}[ht!]
\centering
\begin{tabular}{cccc}
\toprule
Tempo & Banshi               & Time signature                           & Melodic tendencies  \\
\midrule
slow & manban & 4/4 & melismatic\\
\multirow{5}{*}{\includegraphics[width=0.06\textwidth]{figs/shapes/vertical_double_arrow.png}}& zhongsanyan & 4/4 & \multirow{5}{*}{\includegraphics[width=0.06\textwidth]{figs/shapes/vertical_double_arrow.png}} \\
& kuaisanyan & 4/4 & \\
& yuanban & 2/4 & \\
& erliu & 2/4 & \\
& liushui & 1/4 & \\
fast & kuaiban & 1/4 & syllabic \\
\bottomrule
\end{tabular}
\caption{Jingju metred banshi.}
\label{tab:banshi}
\end{table}

The primary or original banshi is called 原板 (pinyin: yuanban), with time signature 4/4. When it is transformed into 1/4, the corresponding banshi is 快板 (pinyin), and the tempo is also accelerated. When yuanban is transformed into 4/4, the resulting banshi is 慢板 (pinyin: manban), meaning slow banshi. When yuanban is transformed into manban, not only the time duration of syllables are extended, but also the number of ornamentations within each syllable are increased. However, when yuanban is converted to kuaiban, the singing style becomes almost syllabic -- one beat for one syllable. 

三眼 (pinyin: sanyan, literally translated as three eyes) is another name for manban. Sanyan banshi can be divided into three sub-banshi -- 慢三眼 (mansanyan, equal to manban), 中三眼 (zhongsanyan), 快三眼 (kuaisanyan). The tempo of kuaisanyan is faster than zhongsanyan, and they both faster than mansanyan but slower than yuanban. Except for yuanban, the banshi of time signature 2/4 category also contain 二六 (pinyin: erliu, literally translated as two six) because their couplet has six accented beats. In terms of shengqiang xipi, there is another metred banshi -- 流水 (pinyin: liushui, literally running water) which uses 1/4 time signature, is faster than yuanban but slower than kuaiban. We list major jingju metred banshi in \tabref{tab:banshi}.

Three main unmetred banshi are 导板 (daoban), 回龙 (huilong) and 哭头 (kutou). Daoban is the first melodic line of a chang duan. Huilong follows daoban, and is used to draw out the metred banshi. Kutou, literally crying head, is used for a grievous outburst and can occur after any section of the couplet. The variety of banshi is needed to convey the emotional content of the lyrics. In general, yuanban is related to neutral and narrative lyrics content; manban reflects introspective, and deep emotions; while kuaiban expresses agitation, nervousness.

The precise, clear pronunciation is critical to jingju listening comprehension, and also form an important aural aesthetic value of jingju. The primary focus of this dissertation is the pronunciation aspect of jingju singing. An in-depth description of jingju pronunciation concepts is given in the following section.

\section{A review of automatic assessment of musical performance}

\subsection{General automatic assessment of musical performance}

Singing voice assessment

\shortcite{Mayor2006b} Old but interesting

Probablistic and rule-based note alignment and expression segmentation.

Their algorithm first alignment the midi note to the singing voice pitch track. They use Viterbi as the alignment algorithm. The cost probablity (similar to emission probability) is not calculated by some GMM models but by heurstic rules. The rules defined on timing, pitch, energy, vibrato and timbre indicate the probabilities of note change and alignment.

The expression segmentation use the similar idea. They defined many rules for attack, sustain, vibrato, release and transition. The HMM topology of the expression is constrait by the note segmentation, thus it is still an alignment problem.

After obtaining the expression segments, they attribute detailed expression labels to the segment, such as scoop-up attack, fall-down release.

The rules are defined by words, so it's not very clear how to implement these rules and how to use these rules as the probabilities.

\shortcite{Nakanoa} They present an assessment method without using any reference MIDI or audio. So the method is suitable for evaluating unknown melodies.

The first feature is to describe the pitch deviation from the semitone pitch grid. The feature is called semitone stability, which is based on a Gaussian filter of the pitch track. The filter amplitude peak is located in the middle of the grid interval. So if a out-of-tune melody pass the filter, the stability will have peak at a high frequency.

The second feature is related to vibrato. They detected the vibrato segment by doing FFT on the pitch track.

The evaluation dataset has 600 songs sung by 12 singers - 6 good and 6 poor.

The SVM is trained in a cross-validation way to classify the good and poor singers.

\shortcite{Caoa} Another paper to find the correlation between objective assessment model with perceptual ratings.

Acoustic measurements:
(1) Intonation accuracy: recursive alignment, DTW between MIDI pitch and singing pitch
(2) Rhythm consistency: cosine distance between midi note sequence and singing note sequence; basic-variation-bias
(3) Timbre brightness: mean frame-wise spectral centroid, 2k-3kHz energy ratio
(4) Vocal clarity: harmonic-to-noise ratio, narrow band to wide band energy ratio

Assessment model:
SVM regression on (1) (2) (3) (4) features and the combination of them.

Singing dataset:
200 clips from 10 subjects, each 15 seconds long

Perceptual experiment:
5 judges professional, 0-10 rating score

Results:
Spearman rank correlation coef, and absolute error between the SVM prediction and perceptual judgement score.

Overall performance, correlation > 0.9
For each aspect (intonation..) > 0.65

\shortcite{Liu2011a} The journal is a little bit weird, but the paper is good.

They introduced a two-step method for solfege assessment.

Step1: note alignment: They use YIN to extract pitch track, then use DTW to align the reference pitch to extracted pitch. To compensate the onset inaccuracy brought by missing consonant pitch track, they used some heuristic rules to adjust the onset.

Step2: singing evaluation on pitch and rhythm. 
Pitch features: absolute pitch deviation and relative pitch interval
Rhythm: consine distance between reference and extracted note durations, cumulative note residual by linear regression of the note onsets, lagged tempo reference (LRT)

Results:
(1) They evaluate onset precision on 993 syllables. The onset adjustment can lead to a smaller onset difference.
(2) The subjective evaluation is conducted by asking 10 experts to listen to 7 songs. Spearman ranked correlation is measured between experts's rating and the features. Relative interval is the most correlated feature for intonation rating and LRT is the most correlated feature for rhythmic rating
.

\shortcite{Tsai2012a} Three perceptual dimension rating: intonation, rhythm and volume.

Method:
Intonation: pitch track + DTW 
Volume: log-energy + DTW
Rhythm: pitch strength + HMM classification
Overall score: linear regression

Remarks:
(1) They mentioned singing power ratio SPR (peak energy ratio between 2-4kHz and 0-2kHz) is good for evaluating operatic singing voice but not pop singing.

(2) Using HMM model the pitch strength time sequence. The out-of-beat HMM is build by shifting the singing voice ahead and behind K samples.

(3) Experiment uses synthesised data (objective) to validate the model.

\shortcite{Molinaa} Low-level and high-level features for singing voice assessment

Target: real human reference singing, not MIDI

Low-level features: TIE (total intonation error) - sum of the cost matrix values of the alignment path; Root mean square the alignment path linear regression

High-level features: They did the note transcription first. The features are note onset difference, frequency difference, interval difference and their note length weighted version.

Dataset: 27 pitch/rhythm random variations (synthesed) of the target singing. 4 musicians subjective ratings on intonation, rhythm and overall rating.

Evaluation: 
(1) interjudgement reliability, 
(2) correlation coefficients between individual feature and rating - TIE and high-level frequency feature works the best for the intonation rating; low-level RMS of alignment path works the best for rhythm rating.
(3) Quadratic polynominal regression of all the features achieve > 0.95 correlation values for all ratings.

\shortcite{Daido2014a} It's about to build a regression model to predict the singing enthusiam.

The dataset contains 36 singers singing on pop song. Each singer sing twice, first time enthusiastically and second time normally. The stimuli for the subjective evaluation are the phrases (segments) in the dataset.

\shortcite{Schramm2015b} Dataset: solo singing voice, reference score, note-level binary annotation (correct or incorrect): pitch, onset, offset.

Method:
(1) DTW reference score to transcribed note alignment. Pitch is transcribed by Pyin, note by a hysteresis approach of Emilio Molina.
(2) For each assessment category: pitch, onset, offset deviations, he constructed the gamma probability density functions for both correct and incorrect class, build a Baysian classifier, then found the fuzzy boundary between the correct and incorrect class.

Evaluation:
Accuracy > 90%
No baseline compared

\shortcite{Guptac} This paper has two parts. The first part generalized the mispronounciation rules for the singing voice of the south asian english dialects. Mostly, mispronunciation of the consonants. See table 3.

The second part use ASR system with an adapted dictionary. This dictionary is a modified version of the native american english dictionary (L2 TIMIT), where the mispronunciated phones are either the substitution or the combination of TIMIT phones.

The model is trained by KALDI, DNN-HMM. Then they forced align the lyrics to the audio. Using the adapted dictionary can better detect the mispronunciation. The result is compared with the one detected using L2 dictionary.

\shortcite{Bozkurta} An interesting work of evaluating the singing performance of the conservatory entrance exam.

The first pointed out the problem that although many automatic singing assessment works have been done, there is no openly available dataset. Thus, the comparative study of different algorithm is not feasible.

They collected a dataset which contains the 2599 piano reference and 1018 singing performance of 40 different melodies. These singing performances are labeled as pass or fail by 3 professors.

They didn't open the dataset because the copyright issue. However, they opened the pitch track extracted from these recordings.

They use DTW to align the corresponding singing performance pitch track and the piano reference pitch track. The basline system uses below features (1) Pitch difference histogram (2) DTW cost, (3) the amount of the length change of the DTW alignment.

The classification model is a simple MLP and they got a 0.74 prediction accuracy.

The interesting part of the paper is the description of the exam, and the human judging procedure.

\shortcite{Guptab} Well-written and clear structure.

High lights:
(1) Follow the singing assessment principle on 6 dimensions:
Intonation accuracy, rhythmic consistency, pronunciaiton, vibrato, volume and pitch range

(2) Using MFCC based DTW alignment to avoid intonation error

(3) Add cognition modeling L6 norm of 1 second frame disturbance, and L2 norm of all the frame disturbance. 

(4) Experiment various systems with different feature combinations

Remarks:
(1) For intonation and rhythm, they only use raw features such as pitch track and MFCCs and didn't use note-level features which would be more musical meaningful. However, one needs to take the risk of the inaccurate note transcription.

(2) Some results in Table V are not logical and not explained by authors. E.g. Why many LR and MLP models results on average score have different trends? Check systems 4,5,6 and systems 6,7,8. Why some average score and average leave one out scores have different trends? Check systems 4,5,6 MLP and system LR 7,8,9 on average score and average leave one out scores.

(3) They claim the system 8 improved the baseline B by 98\% on correlation accuracy. This comparision seems, uhmmm... Normally, we don't directly compare two systems with different input feature dimensions. The feature dimension of Baseline B is 3 but that of system 8 is 16.

Instruments music performance assessment

\shortcite{Robinea} To assess the saxphone notes.

They recorded 6 notes with different dynamics: p, mf, f, cresando/decresando, vibrato from different level players.

They extracted feature for stable notes, cresando/decresando notes and vibratos based on pitch and amplitude. The features are quite simple but their description is hard to understand.

They map the feature scores to the marks. In the evaluation, they are very correlated with the players' level.

The features introduced are contrainted by the specific notes - stable, cresando/decresando and vibratos.

\shortcite{Knighta} This work collected 3 trumpet players and 1 trombone player performance. Three levels of dynamics and 3 musical phrase are recorded. In total 239 notes.

They asked 7 raters to rate these notes, and take the round average score as the groundtruth. The intersubject correlations are quite high for 118 notes but quite low for another 121 ones. The rating scale is from 1 to 7. 

They used 56 features, most of them are spectral features. They used SVM as the classifier.

The best part of the paper is the experiment design.
(1) normal 2, 3, 7 classes classification. All players notes are mixed in the cross-validation folds.

(2) Leave one player out. Train on 3 players and test on another player.

(4) Identify the player experiment.

Results:
(1) 2 classes > 3 > 7, this is obvious
(2) leave one player out experiment is worse than (1). This is also understandable. Because the model might fit on the player's characteristics other than the tone quality. E.g. player 4 has more bad tones than good tones. Player 2 has more good tones than bad tones. The training set containing players 2 and 4 might be fitted by their player qualities. Thus, in the test set, if the player characteristics are more close to player 2, it will be classified as good. However, if it is close to player 4, it is bad.

\shortcite{Hana} detect (1) assembeling error (2) fluctuated sound (3) mis-figuring

Assembeling error affects the pitch tuning, they use HPCP peak histogram to detect the tuning center.

fluctuated sound causes inharmonic partials. They use binary masked spectrogram to highlight these particals, then take the sum across the bins as the feature

Mis-fingering is represented by the timbre problems. So they used MFCC + sparse filtering + RF classication method.

All these features are quite effective for detecting corresponding mistakes.

\shortcite{Luoa} Great paper on violin mistakes detection.

They did features + classification system for violin note-level mistakes detection.

Highlights:
(1) They recorded 26 clips of sucessive legato notes violin playing as the dataset.

(2) Segmentation between notes and within each note (onset, offset, sustain) was achieved using a photo resistor and four rings of surfacemounted light-emitting diodes (SMD LEDs).

\shortcite{Vidwans2017a} score-based features, DTW alignment with the reference score and segmented into notes.
(1) note steadiness
(2) duration histogram
(3) DTW based feature, cost normalized by the DTW length and slope deviation
(4) note insertion ratio
(5) note deletion ratio

score-based and score independed features are the best performed one.

Dataset:
Florida Bandmasters Association

\shortcite{Wua} Using sparse coding unsupervised learning to learn features for the percussive music performance.

Dataset: 274 recordings of middle school snare etudes. Assessment of musicality and rhythmic accuracy. 

Features:
LHM local histogram matrix:
(1) IOI (inter-onset interval) histogram vector
(2) Amplitude histogram vector
(3) Average MFCC vector

They segment the whole music piece into non-overlapped 10s segments, compute the local histogram vectors of above three features and concatenate these vector in both feature and time dimensions.

Baseline:
(1) a bunch of features
(2) statistics of LHM features (crest, skewness, ...)
(3) Sparse code of STFT

Model:
SVR

Metrics:
correlation coef and coef of determination

Results:
Learned features (Sparse code with LHM) achieve comparable results with the designed features,  Finally, combining the designed features with the SC features, the highest performance can be achieved.

\shortcite{Pati2018a} Neural network on music performance assessement.

Dataset: Saxphone, Clarinet and Flute music excerpts, Middle school and symphonic band playing.

Assessment dimensions: (1) Musicality (2) Note accuracy (3) Rhythmic accuracy (4) Tone quality

Network architectures:
(1) Fully-convolutional net (FCN) + Pitch track input
(2) CRNN + mel input
(3) Late-fuse the representations of (1) and (2)

Baseline:
Bunch of features + SVR

What to predict?
The human judagement as the ground truth.

How to predict?
Minimize the mean square error loss between prediction and the ground truth.

How to evaluate?
Coefficient of Determination R2 and Pearson correlation coef

Results:
(1) For symphonic band, net (1) is the best; for middle school, net (3) is the best. They guess as the performing level increases, which is the case of the symphonic band, the pitch is getting more important.

(2) For musicality dimension, all nets work better than the baseline. They said that because the NN can better capture abstract features (musciality).

(3) They found that, on musicality dimension, the NN (1) first layer weights are similar between symphoic band and middle school performings.

(4) They found pitch track salience are more concentrated on silence if the performing has a low musicality.

\begin{landscape}
\begin{table}[ht!]
\ContinuedFloat
\centering
\begin{tabular}{lcc}
\toprule
Authors              & Goal                                          & Methods                                                                                           \\
\midrule
\shortcite{Robinea} & To assess saxophone notes                     & \makecell{Extracting metrics for straight,\\crescendo/decrescendo\\and vibrato notes}                          \\\hline
\shortcite{Mayor2006b}         & Expression categorization and alignment       & \makecell{Rule-based note and\\expression alignment\\using Viterbi algorithm}                                  \\\hline
\shortcite{Nakanoa}        & \makecell{Singing skill evaluation\\for unknown melodies} & \makecell{Building SVM model\\to classify good and bad performance.}                                          \\\hline
\shortcite{Caoa}           & Singing quality evaluation                    & \makecell{Building SVM regression model\\between features and\\human judgement scores.}                        \\\hline
\shortcite{Knighta}        & Trumpet tone quality assessment               & \makecell{Building SVM model\\to classify trumpet tone quality on 7 scales.}                                  \\\hline
\shortcite{Liu2011a}         & Singing evaluation                            & \makecell{Using correlation coefficient\\to select the best features.}                                        \\\hline
\shortcite{Tsai2012a}         & Karaoke singing evaluation                    & \makecell{Building linear regression model\\for predicting the overall score.}                                \\
\bottomrule   
\end{tabular}
\caption{Summary table of the state of the art automatic assessment of musical performance studies.}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[ht!]
\ContinuedFloat
\centering
\begin{tabular}{lcc}
\toprule
Authors              & Goal                                          & Methods                                                                                           \\
\midrule
\shortcite{Molinaa}      & Singing voice assessment                      & \makecell{Building nonlinear regression model\\for predicting the human judgement score.}                     \\\hline
\shortcite{Hana}          & \makecell{Detecting common mistakes\\of flute players}       & \makecell{Using handcrafted features, thresholding\\and Random Forest classifier\\to detect playing mistakes.} \\\hline
\shortcite{Daido2014a}        & Singing enthusiasm evaluation                 & \makecell{Building linear regression model\\for predicting the human judgement\\of singing enthusiasm.}        \\\hline
\shortcite{Luoa}          & \makecell{Detection of common violin\\playing mistakes}      & \makecell{Building SVM classifiers\\for detecting four types\\of violin playing mistakes.}                     \\\hline
\shortcite{Schramm2015b}         & Solfège assessment                            & \makecell{Constructing Gamma probability\\density functions, building Bayesian classifiers\\for each note.}    \\
\bottomrule   
\end{tabular}
\caption{Summary table of the state of the art automatic assessment of musical performance studies. (continued)}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}[ht!]
\ContinuedFloat
\centering
\begin{tabular}{lcc}
\toprule
Authors              & Goal                                          & Methods                                                                                           \\
\midrule
\shortcite{Guptac}        & Singing mispronunciation detection            & \makecell{DNN-HMM lyrics-to-audio\\forced alignment,\\using an adapted dictionary\\to detect mispronunciation.} \\\hline
\shortcite{Vidwans2017a}         & \makecell{Assessment of student\\music performance}          & \makecell{Building SVR regression model\\for predicting the human judgement score.}                           \\\hline
\shortcite{Bozkurta}         & Singing voice assessment                      & \makecell{Building a Multilayer perceptron model\\for predicting the human judgement.}                         \\\hline
\shortcite{Guptab}           & Singing quality evaluation                    & \makecell{Using both linear regression\\and Multilayer perceptron\\for predicting the human judgement.}        \\\hline
\shortcite{Wua}           & \makecell{Percussive\\music performance assessment}        & \makecell{Using sparse coding to learn the feature,\\then building SVR modelfor prediction.}                 \\\hline
\shortcite{Pati2018a}           & Student music performance                     & \makecell{Using fully-convolutional network\\or covolutional recurrent network\\for prediction.}           \\
\bottomrule   
\end{tabular}
\caption{Summary table of the state of the art automatic assessment of musical performance studies. (continued)}
\end{table}
\end{landscape}


\subsection{Musical onset detection}

\subsection{Lyrics-to-audio alignment}

\subsection{Lyrics recognition}

\subsection{Neural acoustic embeddings}